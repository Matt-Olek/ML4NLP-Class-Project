{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 11:32:34,494 - src.main - INFO - Building vectorstore for dataset: rag-datasets/rag-mini-bioasq\n",
      "2025-04-15 11:32:34,499 - src.data.dataset_loader - INFO - Downloading data from hf://datasets/rag-datasets/rag-mini-bioasq/data/passages.parquet/part.0.parquet\n",
      "c:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/rag-datasets/rag-mini-bioasq (Request ID: Root=1-67fe27b4-7230f840361bae483c64c561;813e0f78-6e76-4035-9931-b0cd27ec3c8f)\n\nInternal Error - We're working hard to fix this as soon as possible!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/rag-datasets/rag-mini-bioasq",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vectorstore\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mbuild_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag-datasets/rag-mini-bioasq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_passages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\src\\main.py:29\u001b[39m, in \u001b[36mbuild_vectorstore\u001b[39m\u001b[34m(dataset_name, num_passages)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m     28\u001b[39m loader = DatasetLoader(\u001b[38;5;28mstr\u001b[39m(CACHE_DIR))\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m df_passages = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_passages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m.head(num_passages)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Create documents and vectorstore\u001b[39;00m\n\u001b[32m     32\u001b[39m embedder = DocumentEmbedder(model_name=EMBEDDING_MODEL)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\src\\data\\dataset_loader.py:63\u001b[39m, in \u001b[36mDatasetLoader.load_passages\u001b[39m\u001b[34m(self, dataset_name)\u001b[39m\n\u001b[32m     61\u001b[39m cache_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_passages\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhf://datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/data/passages.parquet/part.0.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_or_cache_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\src\\data\\dataset_loader.py:33\u001b[39m, in \u001b[36mDatasetLoader.load_or_cache_data\u001b[39m\u001b[34m(self, url, cache_name)\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_parquet(cache_path)\n\u001b[32m     32\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m df.to_parquet(cache_path)\n\u001b[32m     35\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData cached to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    268\u001b[39m     path,\n\u001b[32m    269\u001b[39m     filesystem,\n\u001b[32m    270\u001b[39m     storage_options=storage_options,\n\u001b[32m    271\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m )\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     result = pa_table.to_pandas(**to_pandas_kwargs)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1793\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1787\u001b[39m     warnings.warn(\n\u001b[32m   1788\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_legacy_dataset\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1790\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1813\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1348\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[39m\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m   1347\u001b[39m         filesystem = LocalFileSystem(use_mmap=memory_map)\n\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m finfo = \u001b[43mfilesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_file_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m finfo.type == FileType.Directory:\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28mself\u001b[39m._base_dir = path_or_paths\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pyarrow\\_fs.pyx:590\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.get_file_info\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pyarrow\\_fs.pyx:1498\u001b[39m, in \u001b[36mpyarrow._fs._cb_get_file_info\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\pyarrow\\fs.py:322\u001b[39m, in \u001b[36mFSSpecHandler.get_file_info\u001b[39m\u001b[34m(self, paths)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m         info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m    324\u001b[39m         infos.append(FileInfo(path, FileType.NotFound))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_file_system.py:683\u001b[39m, in \u001b[36mHfFileSystem.info\u001b[39m\u001b[34m(self, path, refresh, revision, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, refresh: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    660\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[33;03m    Get information about a file or directory.\u001b[39;00m\n\u001b[32m    662\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    681\u001b[39m \n\u001b[32m    682\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m     resolved_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m     path = resolved_path.unresolve()\n\u001b[32m    685\u001b[39m     expand_info = kwargs.get(\n\u001b[32m    686\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexpand_info\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    687\u001b[39m     )  \u001b[38;5;66;03m# don't expose it as a parameter in the public API to follow the spec\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_file_system.py:209\u001b[39m, in \u001b[36mHfFileSystem.resolve_path\u001b[39m\u001b[34m(self, path, revision)\u001b[39m\n\u001b[32m    207\u001b[39m repo_id = repo_id_with_namespace\n\u001b[32m    208\u001b[39m path_in_repo = path_in_repo_with_namespace\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m repo_and_revision_exist, err = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_repo_and_revision_exist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, (RepositoryNotFoundError, HFValidationError)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_file_system.py:125\u001b[39m, in \u001b[36mHfFileSystem._repo_and_revision_exist\u001b[39m\u001b[34m(self, repo_type, repo_id, revision)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (repo_type, repo_id, revision) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._repo_and_revision_exists_cache:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepo_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_HUB_ETAG_TIMEOUT\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    129\u001b[39m         \u001b[38;5;28mself\u001b[39m._repo_and_revision_exists_cache[(repo_type, repo_id, revision)] = \u001b[38;5;28;01mFalse\u001b[39;00m, e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2807\u001b[39m, in \u001b[36mHfApi.repo_info\u001b[39m\u001b[34m(self, repo_id, revision, repo_type, timeout, files_metadata, expand, token)\u001b[39m\n\u001b[32m   2805\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2806\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported repo type.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2807\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2814\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2665\u001b[39m, in \u001b[36mHfApi.dataset_info\u001b[39m\u001b[34m(self, repo_id, revision, timeout, files_metadata, expand, token)\u001b[39m\n\u001b[32m   2662\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mexpand\u001b[39m\u001b[33m\"\u001b[39m] = expand\n\u001b[32m   2664\u001b[39m r = get_session().get(path, headers=headers, timeout=timeout, params=params)\n\u001b[32m-> \u001b[39m\u001b[32m2665\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2666\u001b[39m data = r.json()\n\u001b[32m   2667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetInfo(**data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matto\\Documents\\GitHub\\ML4NLP-Class-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/rag-datasets/rag-mini-bioasq (Request ID: Root=1-67fe27b4-7230f840361bae483c64c561;813e0f78-6e76-4035-9931-b0cd27ec3c8f)\n\nInternal Error - We're working hard to fix this as soon as possible!"
     ]
    }
   ],
   "source": [
    "from src.main import build_vectorstore\n",
    "\n",
    "build_vectorstore(\"rag-datasets/rag-mini-bioasq\", num_passages=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
